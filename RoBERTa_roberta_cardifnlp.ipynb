{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: evaluate in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: peft in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: ipywidgets in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: torch in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: requests in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: psutil in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipywidgets) (8.12.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: networkx in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: decorator in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: jedi>=0.16 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pure-eval in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets evaluate accelerate peft tqdm ipywidgets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (0.26.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: filelock in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/hice1/apadmanaban7/.local/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f65deeba9e4df1bf9b3478348c5d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neologism_data = pd.read_csv('all_sampled_twitter.csv')\n",
    "neologism_data['label'] = neologism_data['label'].astype(\"int64\")\n",
    "\n",
    "dataset = Dataset.from_pandas(neologism_data)\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "eval_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"@mattduss @chrislhayes if ISIS does gain a fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Sad news! 150 people shot, drowned in Yobe ri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@StephenNolan they all served and died under t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rousey vs. Holm may be the biggest waste of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Amy Schumer is the stereotypical 1st wor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8739</th>\n",
       "      <td>The city experienced a complete blackout after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8740</th>\n",
       "      <td>After the power outage, the city experienced a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8741</th>\n",
       "      <td>The unexpected blackout plunged the city into ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8742</th>\n",
       "      <td>The company's sudden mobilization of resources...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8743</th>\n",
       "      <td>The company's sudden mobilization of resources...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     \"@mattduss @chrislhayes if ISIS does gain a fo...      0\n",
       "1     \"Sad news! 150 people shot, drowned in Yobe ri...      0\n",
       "2     @StephenNolan they all served and died under t...      0\n",
       "3     Rousey vs. Holm may be the biggest waste of a ...      0\n",
       "4     @user Amy Schumer is the stereotypical 1st wor...      0\n",
       "...                                                 ...    ...\n",
       "8739  The city experienced a complete blackout after...      0\n",
       "8740  After the power outage, the city experienced a...      0\n",
       "8741  The unexpected blackout plunged the city into ...      0\n",
       "8742  The company's sudden mobilization of resources...      0\n",
       "8743  The company's sudden mobilization of resources...      0\n",
       "\n",
       "[8744 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neologism_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = f'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "num_labels = 3\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cbcb970b4a44c09e65fd571e6ac65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcd78262cae4463a08d9f770fdc7c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209759eec4704d57bf68c432a627a89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_full_finetune = RobertaTokenizer.from_pretrained(base_model)\n",
    "model_to_train = AutoModelForSequenceClassification.from_pretrained(base_model, id2label=id2label, num_labels=num_labels).to(device)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer_full_finetune(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True).with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_full_finetune, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    eval_strategy='steps',\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    output_dir=\"AkhilaGP/roberta-senti-neologism-full-finetune\",\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "def get_trainer(model):\n",
    "      return  Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=eval_dataset,\n",
    "          data_collator=data_collator,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def evaluate_model(inference_model, dataset):\n",
    "\n",
    "    predictions_full = []\n",
    "    references_full = []\n",
    "    eval_dataloader = DataLoader(dataset.rename_column(\"label\", \"labels\"), batch_size=8, collate_fn=data_collator)\n",
    "    inference_model.to(\"cuda\")\n",
    "    inference_model.eval()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predictions, references = predictions, batch[\"labels\"]\n",
    "        predictions_full.append(predictions)\n",
    "        references_full.append(references)\n",
    "        accuracy_metric.add_batch(predictions=predictions,references=references,)\n",
    "        f1_metric.add_batch(predictions=predictions,references=references,)\n",
    "        precision_metric.add_batch(predictions=predictions,references=references,)\n",
    "        recall_metric.add_batch(predictions=predictions,references=references,)\n",
    "\n",
    "    accuracy = accuracy_metric.compute()\n",
    "    f1 = f1_metric.compute(average=\"weighted\")\n",
    "    precision = precision_metric.compute(average=\"weighted\")\n",
    "    recall = recall_metric.compute(average=\"weighted\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score (macro): {f1}\")\n",
    "    print(f\"Precision (macro): {precision}\")\n",
    "    print(f\"Recall (macro): {recall}\")\n",
    "    return predictions_full, references_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c103deeb1f364d468aac95d907d42a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AkhilaGP/roberta-senti-neologism-full-finetune/commit/1ae917bf9c457331492c5fa88be54a1e0f99880a', commit_message='Upload tokenizer', commit_description='', oid='1ae917bf9c457331492c5fa88be54a1e0f99880a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AkhilaGP/roberta-senti-neologism-full-finetune', endpoint='https://huggingface.co', repo_type='model', repo_id='AkhilaGP/roberta-senti-neologism-full-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_finetuning_trainer = get_trainer(model_to_train)\n",
    "full_finetuning_trainer.train()\n",
    "# full_finetuning_trainer.push_to_hub()\n",
    "# tokenizer_full_finetune.push_to_hub(\"roberta-senti-neologism-full-finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 328/328 [00:25<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {'accuracy': 0.7545731707317073}\n",
      "F1 Score (macro): {'f1': 0.7621121659307027}\n",
      "Precision (macro): {'precision': 0.7784456828044978}\n",
      "Recall (macro): {'recall': 0.7545731707317073}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [00:24<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {'accuracy': 0.7736280487804879}\n",
      "F1 Score (macro): {'f1': 0.7729581944188018}\n",
      "Precision (macro): {'precision': 0.7723608657814851}\n",
      "Recall (macro): {'recall': 0.7736280487804879}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "evaluate_model(AutoModelForSequenceClassification.from_pretrained(base_model, id2label=id2label, num_labels=num_labels).to(device), test_dataset)\n",
    "pred, ref = evaluate_model(AutoModelForSequenceClassification.from_pretrained(\"AkhilaGP/roberta-senti-neologism-full-finetune\", id2label=id2label, num_labels=num_labels).to(device), test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text, model, tokenizer):\n",
    "  inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "  output = model(**inputs)\n",
    "  prediction = output.logits.argmax(dim=-1).item()\n",
    "  print(f'Sentiment: {id2label[prediction]} for {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_og(text, model, tokenizer):\n",
    "  inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "  output = model(**inputs)\n",
    "  prediction = output.logits.argmax(dim=-1).item()\n",
    "  print(f'Sentiment: {id2label[prediction]} for {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: neutral for Listening to the retro playlist filled with 80's synth-pop hits, he was filled by a wave of falstalagia\n",
      "Sentiment: positive for That fit is straight fire, no cap, you're looking mad schmick\n",
      "Sentiment: negative for The candidate's speech was pure clickbait, all sizzle and no steak\n",
      "Sentiment: positive for The workshop was absolute chaos, with ideas flying in every direction, but somehow, in the middle of that whirlwind, we created something ridiculously inspiring.\n",
      "Sentiment: positive for Absolutely love how our talent pool is just bursting with sparkle-genius nepo babies, each one more effortlessly deserving and glow-ready for success than anyone who actually worked for it.\n",
      "Sentiment: negative for That team-building session was an absolute trainwreck of synergy—everyone was crushed by inspiration and overloaded with motivation to the point of absurdity\n",
      "Sentiment: positive for Taylor's new album slaps hard\n",
      "Full\n",
      "Sentiment: positive for Listening to the retro playlist filled with 80's synth-pop hits, he was filled by a wave of falstalagia\n",
      "Sentiment: positive for That fit is straight fire, no cap, you're looking mad schmick\n",
      "Sentiment: negative for The candidate's speech was pure clickbait, all sizzle and no steak\n",
      "Sentiment: positive for The workshop was absolute chaos, with ideas flying in every direction, but somehow, in the middle of that whirlwind, we created something ridiculously inspiring.\n",
      "Sentiment: positive for Absolutely love how our talent pool is just bursting with sparkle-genius nepo babies, each one more effortlessly deserving and glow-ready for success than anyone who actually worked for it.\n",
      "Sentiment: negative for That team-building session was an absolute trainwreck of synergy—everyone was crushed by inspiration and overloaded with motivation to the point of absurdity\n",
      "Sentiment: positive for Taylor's new album slaps hard\n"
     ]
    }
   ],
   "source": [
    "samples = [    \"Listening to the retro playlist filled with 80's synth-pop hits, he was filled by a wave of falstalagia\",\n",
    "        \"That fit is straight fire, no cap, you're looking mad schmick\",\n",
    "        \"The candidate's speech was pure clickbait, all sizzle and no steak\",\n",
    "        \"The workshop was absolute chaos, with ideas flying in every direction, but somehow, in the middle of that whirlwind, we created something ridiculously inspiring.\",\n",
    "        \"Absolutely love how our talent pool is just bursting with sparkle-genius nepo babies, each one more effortlessly deserving and glow-ready for success than anyone who actually worked for it.\",\n",
    "        \"That team-building session was an absolute trainwreck of synergy—everyone was crushed by inspiration and overloaded with motivation to the point of absurdity\",\n",
    "        \"Taylor's new album slaps hard\"]\n",
    "        \n",
    "\n",
    "non_finetuned = AutoModelForSequenceClassification.from_pretrained(base_model, id2label=id2label, num_labels=num_labels).to(device)\n",
    "full_finetuned = AutoModelForSequenceClassification.from_pretrained(\"AkhilaGP/roberta-senti-neologism-full-finetune\", id2label=id2label, num_labels=num_labels).to(device)\n",
    "\n",
    "for i in samples:\n",
    "    classify(i, non_finetuned , tokenizer_full_finetune )\n",
    "\n",
    "print(\"Full\")\n",
    "for i in samples:\n",
    "    classify(i, full_finetuned, tokenizer_full_finetune)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LORA on roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_LORA = TrainingArguments(\n",
    "    eval_strategy='steps',\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    output_dir=\"AkhilaGP/roberta-senti-neologism-LORA\",\n",
    "    push_to_hub=True\n",
    "\n",
    ")\n",
    "\n",
    "def get_trainer_LORA(model):\n",
    "      return  Trainer(\n",
    "          model=model,\n",
    "          args=training_args_LORA,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=eval_dataset,\n",
    "          data_collator=data_collator,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98f45dacf4f4486935909692168fc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AkhilaGP/roberta-senti-neologism-LORA/commit/a2cc36150be77fe25e566cf1516d58ae6ed00171', commit_message='Upload tokenizer', commit_description='', oid='a2cc36150be77fe25e566cf1516d58ae6ed00171', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AkhilaGP/roberta-senti-neologism-LORA', endpoint='https://huggingface.co', repo_type='model', repo_id='AkhilaGP/roberta-senti-neologism-LORA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", \n",
    "                         inference_mode=False, \n",
    "                         r=8, \n",
    "                         lora_alpha=16, \n",
    "                         lora_dropout=0.1)\n",
    "\n",
    "peft_model = get_peft_model(AutoModelForSequenceClassification.from_pretrained(base_model, id2label=id2label).to(device), \n",
    "                            peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "peft_lora_finetuning_trainer = get_trainer_LORA(peft_model)\n",
    "\n",
    "peft_lora_finetuning_trainer.train()\n",
    "peft_lora_finetuning_trainer.push_to_hub()\n",
    "tokenizer_full_finetune.push_to_hub(\"roberta-senti-neologism-LORA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca29bb7410e41d8a7e379c4d4454ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cddb8b269c4443a2c6d498320614e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79b421fd8394e9aad838f3e31975dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43e8b0abd834385aaaa51ceac618ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a871b7e26fc48efa591913190118222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7dcced7fd548bba8866bae6a660a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [00:26<00:00, 12.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {'accuracy': 0.7930640243902439}\n",
      "F1 Score (macro): {'f1': 0.785750786125696}\n",
      "Precision (macro): {'precision': 0.7852043080500367}\n",
      "Recall (macro): {'recall': 0.7930640243902439}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_lora = AutoPeftModelForSequenceClassification.from_pretrained(\"AkhilaGP/roberta-senti-neologism-LORA\", id2label=id2label)\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(\"AkhilaGP/roberta-senti-neologism-LORA\")\n",
    "pred, ref = evaluate_model(roberta_lora, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive for Listening to the retro playlist filled with 80's synth-pop hits, he was filled by a wave of falstalagia\n",
      "Sentiment: positive for That fit is straight fire, no cap, you're looking mad schmick\n",
      "Sentiment: negative for The candidate's speech was pure clickbait, all sizzle and no steak\n",
      "Sentiment: positive for The workshop was absolute chaos, with ideas flying in every direction, but somehow, in the middle of that whirlwind, we created something ridiculously inspiring.\n",
      "Sentiment: positive for Absolutely love how our talent pool is just bursting with sparkle-genius nepo babies, each one more effortlessly deserving and glow-ready for success than anyone who actually worked for it.\n",
      "Sentiment: negative for That team-building session was an absolute trainwreck of synergy—everyone was crushed by inspiration and overloaded with motivation to the point of absurdity\n",
      "Sentiment: positive for Taylor's new album slaps hard\n"
     ]
    }
   ],
   "source": [
    "for i in samples:\n",
    "    classify(i, roberta_lora , tokenizer_lora)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
