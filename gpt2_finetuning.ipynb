{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7713fab7-f092-4117-bf56-0dea8680dbb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: filelock in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: datasets in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: peft in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (4.46.1)\n",
      "Requirement already satisfied: tqdm in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (4.66.6)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (1.1.0)\n",
      "Requirement already satisfied: safetensors in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from transformers->peft) (0.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: evaluate in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: packaging in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: torch in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: numpy in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install evaluate\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ef8ea85-d04d-4217-99a3-21c446bf2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2ForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56c8cf-ef56-4e2f-ba0b-221895411fbf",
   "metadata": {},
   "source": [
    "### Data To Train On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7cdbae-4ce0-4649-b046-b65a3e0f331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neologism_data = pd.read_csv('base_data_non_genz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4747033-8972-4220-8d66-274e2921c85f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neologism_data['sentence'] = neologism_data['sentence'].astype(str)\n",
    "neologism_data['sentiment'] = neologism_data['sentiment'].astype(str)\n",
    "def assign_label(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 2\n",
    "    elif sentiment == 'neutral':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "neologism_data['label'] = neologism_data['sentiment'].apply(assign_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "028cab8a-5e0f-45c8-b30b-2d6093a56f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    neologism_data['sentence'], neologism_data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict({'label': y_train.tolist(), 'sentence': x_train.tolist()}),\n",
    "    'validation': Dataset.from_dict({'label': y_test.tolist(), 'sentence': x_test.tolist()})\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36ef1ce-a882-41ae-bda4-6dc52dc86fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cx</td>\n",
       "      <td>My new phone's cx is unbelievably smooth;  scr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crispr</td>\n",
       "      <td>Scientists are using crispr technology to edit...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>defi</td>\n",
       "      <td>Despite the market volatility,  my defi invest...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oled</td>\n",
       "      <td>Despite the higher price, the oled screen's vi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>longtermism</td>\n",
       "      <td>Despite the immediate crisis, the government's...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>hallucination</td>\n",
       "      <td>Her vivid descriptions of the alien abduction ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>social distancing</td>\n",
       "      <td>Despite the initial inconvenience, social dist...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>twindemic</td>\n",
       "      <td>This year's twindemic of flu and RSV cases ove...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>mald</td>\n",
       "      <td>After losing the championship, he malded spect...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>noob</td>\n",
       "      <td>Despite being a complete noob at coding,  she ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2505 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word                                           sentence  \\\n",
       "0                    cx  My new phone's cx is unbelievably smooth;  scr...   \n",
       "1                crispr  Scientists are using crispr technology to edit...   \n",
       "2                  defi  Despite the market volatility,  my defi invest...   \n",
       "3                  oled  Despite the higher price, the oled screen's vi...   \n",
       "4           longtermism  Despite the immediate crisis, the government's...   \n",
       "...                 ...                                                ...   \n",
       "2500      hallucination  Her vivid descriptions of the alien abduction ...   \n",
       "2501  social distancing  Despite the initial inconvenience, social dist...   \n",
       "2502          twindemic  This year's twindemic of flu and RSV cases ove...   \n",
       "2503               mald  After losing the championship, he malded spect...   \n",
       "2504               noob  Despite being a complete noob at coding,  she ...   \n",
       "\n",
       "     sentiment  label  \n",
       "0     positive      2  \n",
       "1     positive      2  \n",
       "2     positive      2  \n",
       "3     positive      2  \n",
       "4     positive      2  \n",
       "...        ...    ...  \n",
       "2500  negative      0  \n",
       "2501  positive      2  \n",
       "2502  negative      0  \n",
       "2503  negative      0  \n",
       "2504  positive      2  \n",
       "\n",
       "[2505 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neologism_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb900e17-4ef1-40fd-aa0a-f418bc6dac1f",
   "metadata": {},
   "source": [
    "### Data To Test With"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f103aad-fba4-4b9f-8814-3143aa6acacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('the-reddit-dataset-dataset-comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86df355d-0f6d-4048-949d-2df2074d3c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df['body'].dtype)\n",
    "print(reddit_df['sentiment'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e77a73bc-95ef-402b-8f2a-bc573bf88835",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.dropna(subset=['body', 'sentiment'])\n",
    "reddit_df['body'] = reddit_df['body'].astype(str)\n",
    "reddit_df['sentiment'] = reddit_df['sentiment'].astype(float)\n",
    "def assign_label(score):\n",
    "    if score < -0.5:\n",
    "        return 0  # Negative\n",
    "    elif -0.5 <= score <= 0.5:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "reddit_df['label'] = reddit_df['sentiment'].apply(assign_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b0cd34-9a8d-4ce7-847f-d00ff0340040",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_words = neologism_data.word\n",
    "neo_words_set = set(neo_words.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "514d735c-ccaa-4fd2-b085-9baaeddecfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_reddit_df = reddit_df[reddit_df['body'].str.contains('|'.join(neo_words_set), case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644c68d-9adf-48a4-90a2-8fd89555a302",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a60dd1fe-8144-4678-b018-20891e49237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_checkpoint = 'distilbert-base-cased'\n",
    "# model_checkpoint = 'roberta-base' # you can alternatively use roberta-base but this model is bigger thus training will take longer\n",
    "\n",
    "model_checkpoint = 'gpt2'\n",
    "\n",
    "id2label = {0: \"negative\", 1: \"positive\", 2: \"neutral\"}\n",
    "label2id = {\"negative\":0, \"positive\":1, \"neutral\": 2}\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=3, id2label=id2label, label2id=label2id)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "853002f8-d39c-4bc4-8d07-e44a47de3b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc98609-873d-455c-bac4-155632cda484",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fe08707-657f-4e66-aa72-84899c54bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# # add pad token if none exists\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20f4adb9-ce8f-4f54-9b94-300c9daae1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"sentence\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7600bcd-7e93-4fb4-bd8d-ffc76bed1ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a048d2cfb25041f59e264de41fa91e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f98e0ea4adf4156815ea803101e990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/501 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'sentence', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2004\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'sentence', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 501\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f8e85f9-1804-4f49-a783-4da59580ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9a120-580d-470c-a981-7c7e22604865",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a894819-2e9c-4a53-9790-32130c182bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_eval = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c07b9be2-a3f6-4b38-b9e8-6a2bc8aa945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy_eval.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47500035-a555-46e0-83dc-440586d96b7e",
   "metadata": {},
   "source": [
    "### Apply untrained model to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f3761c1-a297-45c8-882e-d74856259810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "Listening to the retro playlist filled with 80's synth-pop hits, he was overwhelmed by a wave of falstalagia. - negative\n",
      "That fit is straight fire, no cap, you're looking mad schmick - negative\n",
      "Taylor swift’s new album just slaps hard. - negative\n",
      "The candidate's speech was pure clickbait, all sizzle and no steak. - negative\n",
      "Absolutely love how our talent pool is just bursting with sparkle-genius nepo babies, each one more deserving for success than anyone who actually worked for it. - negative\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"Listening to the retro playlist filled with 80's synth-pop hits, he was overwhelmed by a wave of falstalagia.\", \"That fit is straight fire, no cap, you're looking mad schmick\", \"Taylor swift’s new album just slaps hard.\", \"The candidate's speech was pure clickbait, all sizzle and no steak.\", \"Absolutely love how our talent pool is just bursting with sparkle-genius nepo babies, each one more deserving for success than anyone who actually worked for it.\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f4dcd77-f41c-4c7f-94a8-81d013c91a8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in reddit_df.iterrows():\n",
    "    text = row['body']\n",
    "    # print(text)\n",
    "    true_label = row['label']  # Assuming the column name for labels is 'label'\n",
    "\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Compute logits\n",
    "    logits = model(inputs).logits\n",
    "\n",
    "    # Convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    # Map the predicted label to human-readable form\n",
    "    predicted_label = predictions.tolist()\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({'text': text, 'true_label': true_label, 'predicted_label': predicted_label})\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "predictions_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80900498-483d-4246-8729-e19b47253e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spatial problem: Suitability of new locations ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Have you tried toying around with GDELT or Ali...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Damn random internet person of whom I know not...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah nice one. Best of luck with the baby. If yo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was about to write and say this shouldn't be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47365</th>\n",
       "      <td>full list here: http://developer.amazonwebserv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47366</th>\n",
       "      <td>This was posted in another thread.\\r\\n\\r\\nhttp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47367</th>\n",
       "      <td>Careful of the licence on this one.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47368</th>\n",
       "      <td>Also a great example of exposing an API with v...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47369</th>\n",
       "      <td>From the overview:\\n\"We have collected packet ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47370 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  true_label  \\\n",
       "0      Spatial problem: Suitability of new locations ...           1   \n",
       "1      Have you tried toying around with GDELT or Ali...           1   \n",
       "2      Damn random internet person of whom I know not...           1   \n",
       "3      Ah nice one. Best of luck with the baby. If yo...           2   \n",
       "4      I was about to write and say this shouldn't be...           1   \n",
       "...                                                  ...         ...   \n",
       "47365  full list here: http://developer.amazonwebserv...           1   \n",
       "47366  This was posted in another thread.\\r\\n\\r\\nhttp...           1   \n",
       "47367                Careful of the licence on this one.           1   \n",
       "47368  Also a great example of exposing an API with v...           2   \n",
       "47369  From the overview:\\n\"We have collected packet ...           2   \n",
       "\n",
       "       predicted_label  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "47365                0  \n",
       "47366                0  \n",
       "47367                0  \n",
       "47368                0  \n",
       "47369                0  \n",
       "\n",
       "[47370 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05ccd4e9-565b-48dd-aaa6-7e4879d5334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions DataFrame:\n",
      "                                                text  true_label  \\\n",
      "0  Spatial problem: Suitability of new locations ...           1   \n",
      "1  Have you tried toying around with GDELT or Ali...           1   \n",
      "2  Damn random internet person of whom I know not...           1   \n",
      "3  Ah nice one. Best of luck with the baby. If yo...           2   \n",
      "4  I was about to write and say this shouldn't be...           1   \n",
      "\n",
      "   predicted_label  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "\n",
      "Accuracy: 0.0762\n"
     ]
    }
   ],
   "source": [
    "accuracy = (predictions_df['true_label'] == predictions_df['predicted_label']).mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions DataFrame:\")\n",
    "print(predictions_df.head())  # Display first few rows of predictions\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff356f78-c9fd-4f2b-8f5b-097cf29c1c08",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4dde538-cd7f-4ab5-a96d-c30f3003822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules=['c_attn', 'c_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1391303-1e16-4d5c-b2b4-799997eff9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=4, target_modules={'c_attn', 'c_proj'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e0d9408-9fc4-4bd3-8d35-4d8217fe01e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 407,808 || all params: 124,849,920 || trainable%: 0.3266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5db78059-e5ae-4807-89db-b58ef6abedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9244ed55-65a4-4c66-8388-55efd87bceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinathsureshkumar/miniconda3/envs/bda/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc8bc705-5dd7-4305-a797-399b2b0fa2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_v/9p5rkkbj79d0bmthxllw7rqm0000gn/T/ipykernel_54463/769452018.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf75f13dbe9d490e9c7533b38ec36a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.722, 'grad_norm': 16.031253814697266, 'learning_rate': 0.0009001996007984033, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b8257a6fa346158ad11556bc14fda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.7844311377245509}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8645496964454651, 'eval_accuracy': {'accuracy': 0.7844311377245509}, 'eval_runtime': 3.5241, 'eval_samples_per_second': 142.164, 'eval_steps_per_second': 35.754, 'epoch': 1.0}\n",
      "{'loss': 0.6233, 'grad_norm': 2.7890474796295166, 'learning_rate': 0.0008003992015968064, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e68bc8de3c4441802e84d272cd6b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.8063872255489022}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7009407877922058, 'eval_accuracy': {'accuracy': 0.8063872255489022}, 'eval_runtime': 3.3741, 'eval_samples_per_second': 148.484, 'eval_steps_per_second': 37.343, 'epoch': 2.0}\n",
      "{'loss': 0.6022, 'grad_norm': 0.2976591885089874, 'learning_rate': 0.0007005988023952096, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ea274ba1724e18abb227800e88b35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.8143712574850299}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1990628242492676, 'eval_accuracy': {'accuracy': 0.8143712574850299}, 'eval_runtime': 3.5338, 'eval_samples_per_second': 141.774, 'eval_steps_per_second': 35.656, 'epoch': 3.0}\n",
      "{'loss': 0.558, 'grad_norm': 7.786545276641846, 'learning_rate': 0.0006007984031936128, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84959fe3b0624265b2c426128233cd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.8143712574850299}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8622522354125977, 'eval_accuracy': {'accuracy': 0.8143712574850299}, 'eval_runtime': 3.6882, 'eval_samples_per_second': 135.839, 'eval_steps_per_second': 34.163, 'epoch': 4.0}\n",
      "{'loss': 0.5231, 'grad_norm': 0.1532604694366455, 'learning_rate': 0.000500998003992016, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb70a609cca54885af08f6fe27d6034d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.7964071856287425}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9570766687393188, 'eval_accuracy': {'accuracy': 0.7964071856287425}, 'eval_runtime': 3.3125, 'eval_samples_per_second': 151.245, 'eval_steps_per_second': 38.038, 'epoch': 5.0}\n",
      "{'loss': 0.4885, 'grad_norm': 12.65489673614502, 'learning_rate': 0.0004011976047904192, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf939beaf94465db4f1f287ab835a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.810379241516966}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8069131374359131, 'eval_accuracy': {'accuracy': 0.810379241516966}, 'eval_runtime': 3.3249, 'eval_samples_per_second': 150.681, 'eval_steps_per_second': 37.896, 'epoch': 6.0}\n",
      "{'loss': 0.4487, 'grad_norm': 20.17593765258789, 'learning_rate': 0.0003013972055888224, 'epoch': 6.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240e67b65a1c4f59ad5b6bedee282cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.7844311377245509}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9719657301902771, 'eval_accuracy': {'accuracy': 0.7844311377245509}, 'eval_runtime': 3.2739, 'eval_samples_per_second': 153.028, 'eval_steps_per_second': 38.486, 'epoch': 7.0}\n",
      "{'loss': 0.4312, 'grad_norm': 52.96881866455078, 'learning_rate': 0.00020159680638722556, 'epoch': 7.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414d50992f6344dcaba202aacc4294d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.810379241516966}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9140940308570862, 'eval_accuracy': {'accuracy': 0.810379241516966}, 'eval_runtime': 3.2412, 'eval_samples_per_second': 154.574, 'eval_steps_per_second': 38.875, 'epoch': 8.0}\n",
      "{'loss': 0.3994, 'grad_norm': 4.675459861755371, 'learning_rate': 0.00010179640718562875, 'epoch': 8.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373ca6d3e299489b956e443c6cf06a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.8063872255489022}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9120633006095886, 'eval_accuracy': {'accuracy': 0.8063872255489022}, 'eval_runtime': 3.3074, 'eval_samples_per_second': 151.479, 'eval_steps_per_second': 38.097, 'epoch': 9.0}\n",
      "{'loss': 0.3739, 'grad_norm': 25.19195556640625, 'learning_rate': 1.996007984031936e-06, 'epoch': 9.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce5e5149b104bd18dbeebb1cf83973c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.8063872255489022}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9734070301055908, 'eval_accuracy': {'accuracy': 0.8063872255489022}, 'eval_runtime': 3.2566, 'eval_samples_per_second': 153.84, 'eval_steps_per_second': 38.69, 'epoch': 10.0}\n",
      "{'train_runtime': 410.8782, 'train_samples_per_second': 48.774, 'train_steps_per_second': 12.193, 'train_loss': 0.5168248210838455, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5010, training_loss=0.5168248210838455, metrics={'train_runtime': 410.8782, 'train_samples_per_second': 48.774, 'train_steps_per_second': 12.193, 'total_flos': 281535628861440.0, 'train_loss': 0.5168248210838455, 'epoch': 10.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5664d1-9bd2-4ce1-bc24-cab5adf80f49",
   "metadata": {},
   "source": [
    "### Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5dc029e-1c16-491d-a3f1-715f9e0adf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "Listening to the retro playlist filled with 80's synth-pop hits, he was overwhelmed by a wave of falstalagia. - negative\n",
      "That fit is straight fire, no cap, you're looking mad schmick - negative\n",
      "Taylor swift’s new album just slaps hard. - negative\n",
      "The candidate's speech was pure clickbait, all sizzle and no steak. - negative\n",
      "Absolutely love how our talent pool is just bursting with sparkle-genius nepo babies, each one more deserving for success than anyone who actually worked for it. - neutral\n"
     ]
    }
   ],
   "source": [
    "model.to('mps')\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\") # moving to mps for Mac (can alternatively do 'cpu')\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a4aadd7-1c9c-45b9-8050-047f409074cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming reddit_df has 'body' for text and 'label' for the true labels\n",
    "results = []\n",
    "\n",
    "# Move the model to MPS (if using a Mac)\n",
    "model.to('mps')\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "# Iterate over the dataframe rows\n",
    "for index, row in fil_reddit_df.iterrows():\n",
    "    text = row['body']\n",
    "    true_label = row['label']  # Assuming column 'label' contains the true labels\n",
    "\n",
    "    # Tokenize the text with padding and truncation\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"mps\")\n",
    "\n",
    "    # Compute logits using the model\n",
    "    logits = model(inputs).logits\n",
    "\n",
    "    # Get the predicted label by finding the index of the max logits\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "\n",
    "    # Map prediction to the corresponding label\n",
    "    predicted_label = id2label[predictions.tolist()[0]]\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': predicted_label\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "predictions_df_new = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb3544f1-202d-4c85-9116-16cc606fad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_new['predicted_label'] = predictions_df_new['predicted_label'].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "102d55a9-85f3-44d6-9de9-7f5c77689dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions DataFrame:\n",
      "                                                text  true_label  \\\n",
      "0  Damn random internet person of whom I know not...           1   \n",
      "1  Ah nice one. Best of luck with the baby. If yo...           2   \n",
      "2  I was about to write and say this shouldn't be...           1   \n",
      "3   I'm not exactly sure how many contracts the E...           1   \n",
      "4  nevermind, found it\\n\\nfor anyone in need:\\n\\n...           1   \n",
      "\n",
      "   predicted_label  \n",
      "0                2  \n",
      "1                2  \n",
      "2                0  \n",
      "3                0  \n",
      "4                2  \n",
      "\n",
      "Accuracy: 0.3602\n"
     ]
    }
   ],
   "source": [
    "accuracy = (predictions_df_new['true_label'] == predictions_df_new['predicted_label']).mean()\n",
    "\n",
    "# Print the DataFrame and accuracy\n",
    "print(\"Predictions DataFrame:\")\n",
    "print(predictions_df_new.head())  # Display the first few rows of predictions\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084bd9e-f7b1-4979-b753-73335ee0cede",
   "metadata": {},
   "source": [
    "### Optional: push model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eb49a-dd0d-4c9e-b9ab-27e06585fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: notebook login\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login() # ensure token gives write access\n",
    "\n",
    "# # # option 2: key login\n",
    "# # from huggingface_hub import login\n",
    "# # write_key = 'hf_' # paste token here\n",
    "# # login(write_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09496307-e253-47e3-a46f-3f28a84c89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_name = 'shawhin' # your hf username or org name\n",
    "# model_id = hf_name + \"/\" + model_checkpoint + \"-lora-text-classification\" # you can name the model whatever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ea581-0ea3-45f3-af21-362e9093ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(model_id) # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487331a-8552-4fb2-867f-985b8fe1d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(model_id) # save trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7feaa-b70e-4b1d-a118-23c616d14639",
   "metadata": {},
   "source": [
    "### Optional: load peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cffa01-25a4-4c86-a7fa-a84353b8caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how to load peft model from hub for inference\n",
    "# config = PeftConfig.from_pretrained(model_id)\n",
    "# inference_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     config.base_model_name_or_path, num_labels=2, id2label=id2label, label2id=label2id\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "# model = PeftModel.from_pretrained(inference_model, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6ed42-8ec3-4343-9e42-405feac052ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
